\typeout{Trabalho final de Aprendizado de Máquina}

\documentclass{article}

\usepackage[brazil]{babel}
%\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ijcai11}

\usepackage{times}
\usepackage[alf]{abntex2cite}
\usepackage{indentfirst}

\title{Trabalho Final - Aprendizado de Máquina e Mineração de Dados}
\author{
Diorge Brognara \\
{\bf Gabriel Silva Trevisan}  \\
{\bf Thiago Miranda} \\
{\bf Wilton Vicente Gonçalves da Cruz} \\
DC -Departamento de Computação \\
UFSCar - Universidade Federal de São Carlos \\
}

\begin{document}

\maketitle

\begin{abstract}
No contexto de aprendizado de máquina, dois problemas são comumente discutidos.
O primeiro é a análise de séries temporais, ou seja, dados condicionados a uma variável tempo.
O outro é o ajuste de hiperparâmetros dos algoritmos de aprendizado para melhor se adequarem aos dados e ao enunciado do problema estudado.
Com o objetivo de analisar o comportamento de algoritmos clássicos em relação a estes dois conceitos,
neste trabalho estudaremos o comportamento dos algoritmos de árvore de decisão, {\it Naïve-Bayes},
regressão linear e de regressão logística para a classificação de séries temporais;
também estudaremos os algoritmos de agrupamento {\it K-Means} e {\it Expectation Maximization} (EM)
e suas sensibilidades aos hiperparâmetros de cada um dos algoritmos.
Para tanto, primeiro será feita uma análise teórica sobre o funcionamento de cada um desses algoritmos,
seus usos mais comuns, e o viés indutivo relacionado a eles.
Percebemos que os algoritmos de classificação lidam bem com dados em séries temporais,
mas que um modelo mais específico para isso, incluindo possivelmente conhecimento de domínio, têm uma acurácia superior.
Ainda é possível notar que os algoritmos de agrupamento são muito sensíveis ao número de grupos que pretende se encontrar,
principalmente quando esse valor é menor que o número real de grupos nos dados,
mas pouco sensíveis aos chutes iniciais de centroides ou médias das distribuições.
\end{abstract}

\section{Introdução}

O aprendizado de máquina tem sido bastante discutido em tempos recentes.
Uma definição comum de aprendizado de máquina é a de que uma máquina aprende uma determinada tarefa $T$,
utilizando alguma métrica de performance $P$, se o sistema melhora sua performance $P$ na tarefa $T$ conforme
uma determina experiência $E$ adquirida. \cite{mitchell06}.
Em um contexto mais prático, dizemos que dada uma função desconhecida em um domínio qualquer, $f : A \to B$,
queremos uma aproximação da função $f$ a partir de exemplos de mapeamentos entre $A$ e $B$,
esses exemplos sendo os dados do nosso aprendizado.
No mundo real, utilizamos o aprendizado de máquina em tarefas como classificação de texto,
detecção de objetos em imagens, sistemas de recomendação automática,
detecção de fraudes em cartão de créditos, mecanismos de pesquisa, entre outros.

Podemos classificar as tarefas de aprendizado de máquina em três grupos,
o aprendizado supervisionado, não-supervisionado, e semissupervisionado.
No aprendizado supervisionado, a variável de interesse (o contra-domínio da função $f$ descrita anteriormente)
é conhecida, chamada de classe. Por exemplo, a classe pode ser um valor binário ``spam/não-spam''
para um conjunto de e-mails. Para que o aprendizado seja supervisionado, todos os exemplos utilizados
no treinamento do algoritmo deve estar marcados com a classe.
Caso somente uma parte dos exemplos esteja marcado com a classe, chamamos o aprendizado de semissupervisionado.
Por fim, no aprendizado não-supervisionado, a classe é uma variável latente, ou seja, não conhecida.

Quando comparamos os diferentes algoritmos existentes para uma determinada tarefa,
devemos levar em consideração vários fatores sobre o próprio problema e sobre os dados existentes,
para que possamos tomar uma decisão sobre quão apropriado é o algoritmo a esta tarefa.
Dentre as características que devem ser estudadas,
podemos citar o {\it overfitting}, ou super-adequação aos dados,
onde o aprendiz se prende demais aos exemplos existentes e é incapaz de generalizar o conhecimento adquirido.
Outra característica é o viés indutivo, que é a estratégia utilizada para selecionar a
hipótese mais provável dentro do espaço de hipóteses possíveis para o algoritmo.

\section{Fundamentação teórica}

Nessa seção, serão discutidos alguns aspectos teóricos dos algoritmos que serão examinados nesse trabalho,
e uma visão geral de alguns conceitos fundamentais para essa discussão.
Além de uma visão geral da ideia por trás de cada algoritmo,
entraremos em detalhes sobre o comportamento de cada algoritmo em séries temporais (para os algoritmos supervisionados),
o viés indutivo de cada um, e sua sensibilidade a hiperparâmetros.

O processo de aprendizado de máquina geralmente envolve a extração de informações mais gerais acerca de um evento com base em um conjunto de instâncias conhecidas desse evento.
Em lógica, o processo de se obter conhecimentos gerais a partir de exemplos específicos é denominado indução.
Como, em geral, um conjunto de instâncias não é suficiente para generalizar todos os casos possíveis,
deve-se haver um conjunto de pressuposições acerca do problema que permitam a generalização.
Categorizamos essas pressuposições em duas formas, o viés de linguagem e o viés indutivo.

O viés de linguagem trata da representação do problema e sua solução;
por exemplo, os algoritmos de árvore de decisão representam a solução através de uma árvore,
e o {\it Naïve-Bayes} representa a solução através de uma equação da máxima {\it a posteriori}.
Muitas vezes, a representação do problema é incapaz de representar todas as possíveis hipóteses da solução,
como as árvores de decisão são incapazes de representar hipóteses que usam combinações lineares dos atributos,
enquanto uma regressão linear é capaz de representar essa combinação.

Já o viés indutivo é mais relacionado à computabilidade e utilidade,
e são as formas heurísticas do algoritmo escolher uma determinada hipótese sobre outra,
ambas representáveis em sua linguagem.
Por exemplo, para uma árvore de decisão sobre três atributos binários, de profundidade exatamente três,
existem $2^3 = 8$ possíveis árvores, mas geralmente só uma é considerada.
Uma das formas que os algoritmos de árvore de decisão usam para escolher essa árvore ``melhor''
é através da entropia, como veremos adiante.
Um algoritmo sem viés indutivo produziria como resultado um número exponencial ou até infinito
de possíveis hipóteses, trazendo pouca informação real sobre o problema,
e muitas vezes impossibilitando o cálculo dos resultados.

Na terminologia de aprendizado de máquina, hiperparâmetros são características do algoritmo de aprendizado de máquina,
não inferidos do conjunto de dados de treinamento, que influenciam diretamente no desempenho do algoritmo de aprendizado de máquina.
Quanto mais sensível a hiperparâmetros for um algoritmo, mais seu desempenho será afetado pelos valores desses hiperparâmetros.
Hiperparâmetros podem ser opcionais ou obrigatórios, sendo que os opcionais simplesmente possuem uma forma heurística
de decisão sobre seu valor.
É comum que se faça uma otimização desses hiperparâmetros para cada problema que se deseja resolver,
já que a configuração ótima se altera de um problema para o outro.


\subsection{Árvore de Decisão}

O algoritmo de árvore de decisão é um algoritmo simbólico de classificação,
muito utilizado pela sua alta interpretabilidade e fácil computação.
Nesse algoritmo, cada valor de atributo ou da variável alvo é visto como uma expressão lógica,
de forma que cada instância é vista como uma conjunção de expressões lógicas.

A saída do algoritmo é uma árvore de decisão,
que pode ser vista como um conjunto de regras que associam um valor de saída (classe) para cada conjunção de entrada.
Essa árvore é obtida a partir do conjunto de dados de treinamento,
verificando-se as frequências dos valores dos atributos com relação aos valores da variável alvo.

Diversos algoritmos existem para a criação dessas árvores, como o ID3, C4.5 e CART.
Embora algumas poucas características mudem entre eles, o funcionamento é bastante similar.
Para simplificar a explicação teórica, utilizaremos o ID3,
embora na prática utilizemos o CART, como implementado na biblioteca {\it scikit-learn}.

Para que a árvore seja criada, uma métrica é associada a cada atributo da base de dados,
métrica essa sendo o viés indutivo do algoritmo.
No caso do ID3, a métrica utilizada é o ganho de informação, definida em função da entropia do conjunto.
A entropia é uma medida de dissimilaridade entre o conjunto real e sua ordenação perfeita,
e pode ser calculada como $H(S) = -(p_+ \log_2(p_+)) - (p_- \log_2(p_-))$ no caso de classificação binária,
onde $p_+$ é a proporção de elementos da classe positiva e $p_-$ a proporção de exemplos da classe negativa.
Devido a algumas deficiências da entropia, como atributos com muitos valores possíveis,
geralmente a métrica é um pouco mais modificado, utilizando valores como o ganho de informação,
razão do ganho de informação, ou a métrica Gini.
\cite{classification-and-regression-trees}.

Alguns dos algoritmos também possuem um mecanismo de poda, que busca eliminar ramos da árvore que fornecem menos informações para a classificação das instâncias.
Há diversas maneiras de realizar a poda em árvores de decisão,
uma delas consiste em separar um conjunto de dados de teste,
representar a árvore de decisão como um conjunto de regras do tipo se então,
em seguida, para cada antecedente de cada regra, comparar o desempenho da árvore no conjunto de teste com o desempenho dela,
retirando-se esse antecedente. Caso o desempenho da árvore melhore, esse antecedente é retirado da regra.
Isso diminui o tamanho dos ramos da árvore de decisão, tornando-a menos suscetível ao {\it overfitting}.

Em vários dos algoritmos, ainda existe uma tendência para a escolha de árvores mais curtas,
que também faz parte do viés indutivo dos algoritmos.
Essa escolha é baseada no princípio da navalha de Occam,
que afirma que modelos mais simples são geralmente mais efetivos.
Uma árvore mais curta é, de fato, menos sensível ao {\it overfitting}
devido ao viés de linguagem, pois a representação possibilita menos hipóteses possíveis.

Sobre os hiperparâmetros, as árvores de decisão são geralmente chamadas de algoritmos não-paramétricos.
No entanto, algumas das características que são assumidas por certas versões podem ser manipuladas
a fim de alterar o funcionamento do algoritmo.
Por exemplo, a utilização da poda, e qual forma de poda é utilizada,
podem alterar significativamente o modelo gerado pelo algoritmo.
Algumas implementações ainda permitem a mudança do critério de escolha de atributos,
por exemplo alterando entre a entropia e o Gini.
Um critério bastante utilizado e limitar a profundidade máxima da árvore,
forçando a geração de modelos mais simples e fáceis de serem interpretados.


\subsection{{Naïve-Bayes}}

O algoritmo {\it Naïve-Bayes} é um algoritmo probabilístico de classificação.
Ele utiliza o teorema de Bayes e a pressuposição de independência condicional
dos atributos dada a classe para estimar a probabilidade de cada classe a partir do conjunto de treinamento,
dos atributos do dado observado e de algum conhecimento a priori sobre a distribuição da classe.
É necessário que seja assumida essa independência condicional dos atributos para
que o processo seja computacionalmente executável.

O {\it Naïve-Bayes} é geralmente é considerado um algoritmo {\it lazy} por não ser necessária a construção de um modelo,
ainda que um modelo probabilístico seja gerado.
Em contrapartida, o algoritmo de árvore de decisão mencionado acima é considerado um algoritmo {\it eager},
pois gera um modelo, sendo a árvore o modelo gerado.
Dessa forma, o algoritmo de {\it Naïve-Bayes} deve ser generalizado toda vez que um dado de entrada é recebido.
Isso faz com que o tempo de treinamento seja reduzido, porém gasta-se mais tempo na classificação.
Algumas implementações do {\it Naïve-Bayes} são capazes de transformar o modelo em {\it eager}
caso seja necessário.

O classificador {\it Naïve-Bayes} funciona da seguinte forma:
tendo-se o conjunto de dados de treinamento e dada uma instância não classificada,
o classificador irá atribuir a essa instância a classe que obtiver a maior probabilidade,
dados os atributos da instância, a partir do conjunto de dados de treinamento.
Essa probabilidade é denominada {\it a posteriori}.
Para tanto, é utilizado o teorema de Bayes (equação \ref{eq:bayes}).

\begin{equation}
P(h|D) = \frac{P(D|h) P(h)}{P(D)}
\label{eq:bayes}
\end{equation}

Chama-se de hipótese de máxima {\it a posteriori} $c_{MAP}$ a hipótese $h$ que maximiza o valor $P(h|D)$.
Se considerarmos as hipóteses de que uma nova instância pertença a cada uma das possíveis classes $c$
(do conjunto de classes possíveis $C$),
e $D$ sendo os atributos da nossa base de dados,
procuramos a classe $c$ que maximiza a expressão da equação \ref{eq:cmap}.

\begin{equation}
c_{MAP} = \operatorname*{arg\,max}_{c \in C} P(c|D)
= \operatorname*{arg\,max}_{c \in C} P(D|c) P(c)
\label{eq:cmap}
\end{equation}

O termo $P(c)$ também é chamado de {\it a priori} e representa o conhecimento que se tem antecipadamente sobre a distribuição da classe.
Geralmente, essa {\it a priori} é obtida a partir de algum conhecimento sobre o domínio do problema ou supondo-se equiprobabilidade.
No caso da equiprobabilidade, torna-se necessário maximizar apenas o primeiro fator.

A suposição de independência condicional feita pelo {\it Naïve-Bayes} torna mais simples a determinação do termo $P(D|c)$.
A partir dessa independência, é possível determinar a probabilidade de cada atributo separadamente e, em seguida, multiplicá-los para obter o valor do termo em questão.

Para determinar o valor de cada $P(a_i|c)$, o algoritmo utiliza uma abordagem frequentista,
ao contar a frequência relativa de cada um dos termos.
No caso de variáveis não-contáveis (contínuas),
o algoritmo pode supor uma determinada distribuição dos dados,
sendo que uma convenção muito comum é assumir a distribuição gaussiana caso não haja
conhecimento de domínio mais adequado.

Para evitar que algum dos valores de $P(a_i|c)$ afete demasiadamente a resposta do algoritmo
devido a um termo ser inexistente, provocando uma multiplicação por zero,
é comum utilizar o chamado {\it M-Estimator}.
A função dessa suavização é adicionar um número $M$ de ocorrências a todos os possíveis termos,
de forma que não haja uma tendência muito grande a seguir os dados,
ou seja, para evitar o {\it overfitting}.
O valor mais comum é $M=1$.

Notamos que esse classificador possui uma resposta bastante determinística:
existe somente uma hipótese que maximiza a {\it a posteriori}.
E esse é exatamente o viés indutivo do método,
a ``busca'' é sempre guiada para a resposta ``MAP''.

\subsection{Regressão}

Regressão é um método estatístico cujo objetivo consiste em inferir a relação entre uma variável dependente com outras variáveis independentes.
Em outras palavras, um conjunto de variáveis explicam outra variável, permitindo a descrição de um conjunto de dados.
Na área de aprendizado de máquina, também é chamado de regressão o problema canônico
que visa predizer o valor de uma variável contínua utilizando o aprendizado supervisionado.

Existem várias formas de se aplicar uma regressão,
duas das mais utilizadas sendo a regressão linear e a regressão logística.
Esses nomes se referem ao modelo utilizados para descrever a predição,
com a regressão linear tentando descrever os dados através de uma reta,
e a regressão logística utilizando a função logística ou sigmoide.
Isso não implica no método ou algoritmo de resolução;
por exemplo, a regressão linear pode ser feita
através do famoso método dos mínimos quadrados,
ou através de um algoritmo de gradiente descendente.

Regressões são amplamente utilizadas na prática,
devido a sua simplicidade e interpretabilidade,
unida com uma rápida execução, resultados satisfatórios (apesar de nem sempre os melhores),
e de utilizarem o robusto arcabouço estatístico existente,
onde é possível calcular taxas de erro e outras métricas que outros algoritmos podem não ter.

O viés indutivo da regressão depende do método de resolução utilizado.
Por exemplo, o método dos mínimos quadrados busca o mínimo global
do quadrado do resíduo da predição, resíduo esse representado na expressão \ref{eq:mmq},
onde $\hat{y}$ é o valor predito e $y$ o valor real.
A busca por gradiente descendente, por outro lado,
também busca minimizar o quadrado do resíduo da predição da mesma expressão \ref{eq:mmq},
mas faz uma busca local guiada pela derivada da função estimada.

\begin{equation}
\operatorname*{arg\,min}_{\hat{y}} (\hat{y} - y)^2
\label{eq:mmq}
\end{equation}


\subsection{{K-Means}}

{\it K-Means} é um algoritmo não-supervisionado de agrupamento, com uma abordagem geométrica.
A ideia dele é que elementos próximos, segundo uma função-distância, são parecidos e, portanto, pertencem a um mesmo grupo.
É importante observar que a versão original do {\it K-Means} só tem garantia de convergência se a função-distância utilizada for a euclidiana.

O {\it K-Means} é utilizado onde quando se conhece o número de grupos que dividem o conjunto de dados.
Basicamente, o algoritmo consiste em definir o número $K$ de grupos a serem obtidos (parâmetro do algoritmo),
e um chute inicial de $K$ vetores, chamados de centroides,
que representam o centro geométrico dos grupos.
Cada instância de treinamento é classificada de acordo com o centroide mais próximo pela função-distância,
e então o centroide é reajustado para o novo centro das instâncias que classificou.
Esse processo é repetido iterativamente até que nenhuma instância mude sua classificação durante uma iteração.
Os pontos ``pertencentes'' a cada centroide é então considerado um grupo,
e os centroides são descartados.

Sendo uma abordagem puramente geométrica,
e tradicionalmente pautada na distância euclidiana,
o algoritmo tem um viés indutivo simples,
de encontrar os centroides que minimizem a distância entre cada instância e o centroide mais próximo.
No entanto, uma característica mais interessante é a escolha dos centroides iniciais.
Os centroides iniciais são escolhidos aleatoriamente,
e diversos trabalhos propõe escolhas melhores para esses pontos,
como o algoritmo {\it K-Means++}, com ganhos significativos de performance.
Portanto, o algoritmo {\it K-Means} possui um viés indutivo
aleatório, baseado na escolha de centroides iniciais.

\subsection{{Expectation Maximization}}

{\it Expectation Maximization} (EM) é um algoritmo com abordagem probabilística, que pode ser usado para a tarefa de agrupamento. O algoritmo é iterativo, com cada iteração dividida em duas etapas principais, chamadas de {\it Expectation} (E) e {\it Maximization} (M), que originam o nome do algoritmo.

O EM busca maximizar o valor esperado da probabilidade de um conjunto de parâmetros, em relação a um conjunto de variáveis observadas e um conjunto de variáveis não observadas \cite{Singh}. O algoritmo possui garantia de convergência \cite{DLR1977}, porém, assim como ocorre com o k-Means, ele pode convergir em ótimos locais.

Dados um conjunto de dados observados $X$, um conjunto de dados não observado $Z$ e um conjunto de parâmetros $\theta$ que se deseja estimar, os seguintes passos são repetidos até a convergência do algoritmo:

\begin{description}
\item[Passo E] Determinar $P(X|\theta) = \sum_Z P(X,Z|\theta)$
\item[Passo M] Maximizar a expressão com relação a $\theta$
\end{description}

Para simplificar a computação, é comum, no passo E, calcular o logaritmo da probabilidade, já que os produtos da probabilidades se tornam somas com o logaritmo. Outra simplificação muitas vezes feita é estimar um limitante inferior para a probabilidade, de forma que, maximizando-se o limitante inferior, maximiza-se também a probabilidade, com redução no custo computacional \cite{Dellaert}.

Um viés indutivo do algoritmo é que os parâmetros são determinados a partir dos que obtiverem máxima verossimilhança (probabilidade dos dados condicionados pelos parâmetros).

Dado um modelo probabilístico relacionando os parâmetros e os dados de treinamento, o EM tentará otimizar esses parâmetros. Os tipos de parâmetros a serem otimizados e as distribuições de probabilidade dos parâmetros são provenientes do modelo de dados informado ao algoritmo. Dessa forma, esse modelo de dados é um hiperparâmetro de grande importância para o EM.

O EM pode ser utilizado para problemas de agrupamento, fazendo-se com que os parâmetros sejam os grupos possíveis. Há mais de uma forma de se construir um modelo de probabilidade para os grupos. Uma delas, utilizada para dados categóricos, consiste em informar uma hipótese inicial dos grupos de cada instância e, a partir desses grupos iniciais, começar as iterações. Outra forma, utilizada para atributos numéricos, é assumir a probabilidade de cada grupo como sendo uma gaussiana no espaço de atributos, com média e desvio padrão iniciais determinados manualmente ou aleatoriamente.

O algoritmo EM pode realizar o agrupamento de duas formas, uma delas é conhecida como {\it Hard EM} e a outra é conhecida como {\it Soft EM} \cite{mcallester}. Na abordagem {\it Hard EM}, cada instância pode pertencer a apenas um grupo, enquanto na abordagem {\it Soft EM}, uma instância pode pertencer a mais de um grupo.

\section{Experimentos e Resultados}

\subsection{Algoritmos Supervisionados}

\subsubsection{Árvore de Decisão}

\subsubsection{{\b \it Naïve-Bayes}}

\subsubsection{Regressão}

\subsection{Algoritmos não Supervisionados}

\subsubsection{{\b \it k-Means}}

\subsubsection{{\b \it Expectation Maximization}}

\section{Conclusões}

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliography{main}

\end{document}